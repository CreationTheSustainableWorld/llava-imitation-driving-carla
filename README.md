# ğŸš— LLaVA-based Autonomous Driving Agent in CARLA

This project demonstrates a simple imitation learning pipeline using [LLaVA 1.5](https://github.com/haotian-liu/LLaVA) (a vision-language model) combined with a custom-trained MLP to control a vehicle in the CARLA simulator.

![HUD Screenshot](images/hud_screenshot.png)

---

## ğŸ¯ Overview

- Vision model: **llava-hf/llava-1.5-7b-hf**  
- Action model: **Custom MLP (linear layers)**  
- Input: Image + Prompt  
- Output: Continuous actions: **steer, throttle, brake**

The system observes RGB camera images in a simulated environment and makes real-time driving decisions based on visual and contextual information.

---

## ğŸ§  Model Architecture

```text
Camera Image + Prompt
        â†“
    LLaVA (frozen)
        â†“
Last hidden state [CLS] (dim=4096)
        â†“
   MLP Head (4096 â†’ 512 â†’ 3)
        â†“
[Steer, Throttle, Brake]
```

---

## ğŸ–¼ï¸ Demo

![Frame Sample](images/example_frame.png)

---

## ğŸ› ï¸ Requirements

```bash
pip install torch transformers carla pygame Pillow numpy
```

---

## ğŸš€ How to Run

1. Launch CARLA simulator (`CarlaUE4.exe`)
2. Then run:

```bash
python run_carla_imitation_agent.py
```

> You should see the vehicle driving based on visual input and the HUD displaying its control states.

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ run_carla_imitation_agent.py       # Main execution script
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ mlp_action_head.pth            # Trained MLP
â”‚   â””â”€â”€ llava_config/                  # (optional) LLaVA model weights
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ hud_screenshot.png             # HUD screenshot
â”‚   â””â”€â”€ example_frame.png              # Camera image sample
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ actions.json                   # Saved control logs
â”‚   â””â”€â”€ log_video.mp4                  # (optional) driving video
â””â”€â”€ README.md
```

---

## ğŸ¤– Training Summary

- Training data was collected from a behavior agent in CARLA
- Each image was paired with its corresponding `[steer, throttle, brake]` vector
- LLaVA was frozen, and only the MLP was trained

---

## ğŸ“Œ Note

This is a lightweight imitation learning demonstration. You can:
- Improve precision by fine-tuning LLaVA
- Add multi-camera views
- Incorporate memory or language-based reasoning

---

## ğŸ‘¤ Author

**Daiki Matsuba**  
- GitHub: [CreationTheSustainableWorld](https://github.com/CreationTheSustainableWorld)  
- Portfolio: [Google Sites](https://sites.google.com/view/job-application-portfolio)
