# ğŸš— LLaVA-based Autonomous Driving Agent in CARLA

This project implements a vision-language imitation learning agent in CARLA using LLaVA and a custom MLP action head.

## ğŸ¯ Features
- ğŸ” Vision encoder: LLaVA 1.5 (7B)
- ğŸ¤– Action head: Custom MLP trained on real CARLA driving data
- ğŸ® Real-time control in CARLA
- ğŸ“º HUD and camera view rendered with pygame
- ğŸ“¦ All models and scripts are ready to run

## ğŸ–¼ï¸ Sample Output
![HUD View](images/hud_screenshot.png)

## ğŸ§  Model Architecture
LLaVA (image + prompt) â†’ last hidden state â†’ MLP â†’ [steer, throttle, brake]

## ğŸ› ï¸ Requirements
```bash
pip install -r requirements.txt
