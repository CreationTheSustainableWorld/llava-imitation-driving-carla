# ğŸš— LLaVA-Based Vision-to-Action Imitation Model for Autonomous Driving

This project demonstrates how to repurpose the [LLaVA-1.5-7B](https://huggingface.co/llava-hf/llava-1.5-7b-hf) multimodal model for **real-time driving control** using **CARLA simulator**.  
By extracting intermediate visual features from LLaVA and training a lightweight MLP, we achieve image â†’ action mapping for autonomous vehicles.

---

## ğŸ“Œ Overview

```
Image (RGB) + Prompt
     â†“
[LLaVA: vision encoder + language decoder]
     â†“ (CLS token)
Intermediate Visual Feature
     â†“
[MLP Head]
     â†“
Action: steer, throttle, brake
```

---

## âœ… Key Features

- ğŸ”„ **Modular design**: Feature extraction, training, and inference are fully decoupled.
- ğŸ¯ **Practical structure**: End-to-end flow from CARLA simulation â†’ data collection â†’ training â†’ real-time control.
- ğŸ§  **Creative adaptation**: LLaVA is used not for text generation but for decision-making features.
- ğŸ§ª **Tested in CARLA**: Live vehicle control via real-time inference.

---

## ğŸ› ï¸ Folder Structure

```
.
â”œâ”€â”€ data_collection/              # CARLA agent script for image-action collection
â”œâ”€â”€ features_extraction/         # Extract LLaVA intermediate features (.pt)
â”œâ”€â”€ mlp_training/                # Train MLP from features to actions
â”œâ”€â”€ real_time_control/           # Inference script for CARLA real-time driving
â”œâ”€â”€ trained_models/              # Saved MLP model
â”œâ”€â”€ _out/                        # Collected images and actions
â””â”€â”€ README.md
```

---

## ğŸ“‚ Components

| Folder | Description |
|--------|-------------|
| `data_collection/` | Records driving images + actions from CARLA |
| `features_extraction/` | Uses LLaVA to extract CLS token from each image |
| `mlp_training/` | Trains MLP (ActionHead) on features-action pairs |
| `real_time_control/` | Loads LLaVA + MLP and controls CARLA vehicle in real time |

---

## ğŸ“ˆ Improvements & Extensions

| Challenge | Future Direction |
|----------|------------------|
| Only MLP | Use Transformer or CNN-based ActionHead |
| No temporal context | Add LSTM/GRU for time-series modeling |
| No reward usage | Apply RL fine-tuning (e.g., PPO, TD3+BC) |
| Limited visualization | Add training/inference visualization dashboards |

---

## ğŸ“½ï¸ Demo

_ğŸ§© Coming Soon: Demo video of agent driving in CARLA._

---

## ğŸ“š References

- [LLaVA Model (Hugging Face)](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- [CARLA Simulator](https://carla.org/)
- [TD3+BC: Offline Reinforcement Learning](https://arxiv.org/abs/2106.06860)

---

## ğŸ‘¤ Author

GitHub: [CreationTheSustainableWorld](https://github.com/CreationTheSustainableWorld)
